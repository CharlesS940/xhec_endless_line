{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attendance = pd.read_csv(\"data/attendance.csv\")\n",
    "entity_schedule = pd.read_csv(\"data/entity_schedule.csv\")\n",
    "link_atrraction_park = pd.read_csv(\"data/link_attraction_park.csv\")\n",
    "waiting_times = pd.read_csv(\"data/waiting_times.csv\")\n",
    "weather_data = pd.read_csv(\"data/weather_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for only Port Aventura rides\n",
    "filtered_attractions = link_atrraction_park[\n",
    "    link_atrraction_park[\"ATTRACTION;PARK\"].str.contains(\"PortAventura World\")\n",
    "]\n",
    "ride_names = filtered_attractions[\"ATTRACTION;PARK\"].str.split(\";\", expand=True)[0]\n",
    "waiting_times = waiting_times[\n",
    "    waiting_times[\"ENTITY_DESCRIPTION_SHORT\"].isin(ride_names)\n",
    "]\n",
    "\n",
    "# Converting to datetime\n",
    "entity_schedule[\"DEB_TIME\"] = pd.to_datetime(entity_schedule[\"DEB_TIME\"])\n",
    "entity_schedule[\"FIN_TIME\"] = pd.to_datetime(entity_schedule[\"FIN_TIME\"])\n",
    "\n",
    "waiting_times[\"DEB_TIME\"] = pd.to_datetime(waiting_times[\"DEB_TIME\"])\n",
    "waiting_times[\"FIN_TIME\"] = pd.to_datetime(waiting_times[\"FIN_TIME\"])\n",
    "\n",
    "# Excluding Tivoli Gardens park and Tivoli Gardens rides\n",
    "entity_schedule = entity_schedule[\n",
    "    entity_schedule[\"ENTITY_DESCRIPTION_SHORT\"] != \"Tivoli Gardens\"\n",
    "]\n",
    "park_closures = entity_schedule[entity_schedule[\"ENTITY_TYPE\"] == \"PARK\"]\n",
    "\n",
    "entity_schedule_rides = entity_schedule[\n",
    "    entity_schedule[\"ENTITY_DESCRIPTION_SHORT\"].isin(ride_names)\n",
    "]\n",
    "ride_closures = entity_schedule[entity_schedule[\"ENTITY_TYPE\"] == \"ATTR\"]\n",
    "\n",
    "# Create an interval index for park closures\n",
    "park_intervals = pd.IntervalIndex.from_arrays(\n",
    "    park_closures[\"DEB_TIME\"], park_closures[\"FIN_TIME\"], closed=\"both\"\n",
    ")\n",
    "\n",
    "# Find which waiting_times fall into any park closure\n",
    "mask_park = waiting_times[\"DEB_TIME\"].apply(lambda x: park_intervals.contains(x).any())\n",
    "\n",
    "# Remove affected waiting times\n",
    "waiting_times = waiting_times[~mask_park]\n",
    "\n",
    "# Initialize mask for ride closures\n",
    "mask_ride = pd.Series(False, index=waiting_times.index)\n",
    "\n",
    "# Loop through each ride and apply the interval check\n",
    "for ride_name, ride_group in ride_closures.groupby(\"ENTITY_DESCRIPTION_SHORT\"):\n",
    "    # Create an IntervalIndex for the ride closure times\n",
    "    ride_intervals = pd.IntervalIndex.from_arrays(\n",
    "        ride_group[\"DEB_TIME\"], ride_group[\"FIN_TIME\"], closed=\"both\"\n",
    "    )\n",
    "\n",
    "    # Mask waiting times for this specific ride that fall within the ride closure interval\n",
    "    mask_ride_for_ride = waiting_times[\n",
    "        (waiting_times[\"ENTITY_DESCRIPTION_SHORT\"] == ride_name)\n",
    "        & waiting_times[\"DEB_TIME\"].apply(lambda x: ride_intervals.contains(x).any())\n",
    "    ].index\n",
    "\n",
    "    # Update the global mask\n",
    "    mask_ride.loc[mask_ride_for_ride] = True\n",
    "\n",
    "# Remove waiting times affected by ride closures\n",
    "waiting_times = waiting_times[~mask_ride]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3387/63129790.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_weather_data['dt_iso'] = relevant_weather_data['dt_iso'].str.split('+').str[0]\n",
      "/tmp/ipykernel_3387/63129790.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_weather_data['date'] = pd.to_datetime(relevant_weather_data['dt_iso'].str.split(' ').str[0])\n",
      "/tmp/ipykernel_3387/63129790.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_weather_data['time'] = relevant_weather_data['dt_iso'].str.split(' ').str[1]\n",
      "/tmp/ipykernel_3387/63129790.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_weather_data['hour'] = relevant_weather_data['time'].str.split(':').str[0].astype(int) + 1\n"
     ]
    }
   ],
   "source": [
    "# Processing weather data\n",
    "relevant_weather_data = weather_data[\n",
    "    [\"dt_iso\", \"temp\", \"humidity\", \"wind_speed\", \"clouds_all\", \"rain_1h\", \"snow_1h\"]\n",
    "]\n",
    "relevant_weather_data[\"dt_iso\"] = relevant_weather_data[\"dt_iso\"].str.split(\"+\").str[0]\n",
    "relevant_weather_data[\"date\"] = pd.to_datetime(\n",
    "    relevant_weather_data[\"dt_iso\"].str.split(\" \").str[0]\n",
    ")\n",
    "relevant_weather_data[\"time\"] = relevant_weather_data[\"dt_iso\"].str.split(\" \").str[1]\n",
    "relevant_weather_data[\"hour\"] = (\n",
    "    relevant_weather_data[\"time\"].str.split(\":\").str[0].astype(int) + 1\n",
    ")\n",
    "relevant_weather_data = relevant_weather_data.drop(columns=[\"dt_iso\", \"time\"])\n",
    "relevant_weather_data.fillna(0, inplace=True)\n",
    "\n",
    "# One hot encoding times\n",
    "waiting_times[\"DEB_TIME\"] = waiting_times[\"DEB_TIME\"].astype(str)\n",
    "waiting_times[\"date\"] = pd.to_datetime(waiting_times[\"DEB_TIME\"].str.split(\" \").str[0])\n",
    "waiting_times[\"time\"] = waiting_times[\"DEB_TIME\"].str.split(\" \").str[1]\n",
    "\n",
    "# Encode the hour and quarter linearly in the time column\n",
    "waiting_times[\"hour\"] = waiting_times[\"time\"].str.split(\":\").str[0].astype(int)\n",
    "waiting_times[\"minute\"] = waiting_times[\"time\"].str.split(\":\").str[1].astype(int)\n",
    "waiting_times[\"time_encoded\"] = waiting_times[\"hour\"] * 4 + waiting_times[\"minute\"] / 15\n",
    "\n",
    "# Merge the weather data\n",
    "waiting_times = pd.merge(\n",
    "    waiting_times, relevant_weather_data, on=[\"date\", \"hour\"], how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "waiting_times[\"day_of_week\"] = waiting_times[\"date\"].dt.dayofweek\n",
    "waiting_times[\"day_of_week_sin\"] = np.sin(2 * np.pi * waiting_times[\"day_of_week\"] / 7)\n",
    "waiting_times[\"day_of_week_cos\"] = np.cos(2 * np.pi * waiting_times[\"day_of_week\"] / 7)\n",
    "\n",
    "waiting_times[\"year\"] = waiting_times[\"date\"].dt.year\n",
    "waiting_times[\"month\"] = waiting_times[\"date\"].dt.month\n",
    "waiting_times[\"day\"] = waiting_times[\"date\"].dt.day\n",
    "waiting_times = pd.get_dummies(waiting_times, columns=[\"year\", \"month\", \"day\"])\n",
    "\n",
    "waiting_times.drop(columns=[\"time\", \"hour\", \"minute\", \"day_of_week\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding ride names\n",
    "ride_names = waiting_times[\"ENTITY_DESCRIPTION_SHORT\"].unique()\n",
    "ride_names_encoded = pd.get_dummies(waiting_times[\"ENTITY_DESCRIPTION_SHORT\"])\n",
    "waiting_times = pd.concat([waiting_times, ride_names_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any rows with downtime for training\n",
    "waiting_times = waiting_times[waiting_times[\"DOWNTIME\"] == 0]\n",
    "\n",
    "# Dropping columns that are not useful for training\n",
    "waiting_times.drop(\n",
    "    columns=[\n",
    "        \"WORK_DATE\",\n",
    "        \"FIN_TIME\",\n",
    "        \"NB_UNITS\",\n",
    "        \"GUEST_CARRIED\",\n",
    "        \"NB_MAX_UNIT\",\n",
    "        \"ADJUST_CAPACITY\",\n",
    "        \"OPEN_TIME\",\n",
    "        \"UP_TIME\",\n",
    "        \"CAPACITY\",\n",
    "        \"DOWNTIME\",\n",
    "        \"ENTITY_DESCRIPTION_SHORT\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding attendance as a feature\n",
    "attendance = attendance[attendance[\"FACILITY_NAME\"] == \"PortAventura Park\"].rename(\n",
    "    columns={\"USAGE_DATE\": \"date\"}\n",
    ")\n",
    "waiting_times = pd.merge(waiting_times, attendance, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DEB_TIME', 'DEB_TIME_HOUR', 'WAIT_TIME_MAX', 'date', 'time_encoded',\n",
       "       'temp', 'humidity', 'wind_speed', 'clouds_all', 'rain_1h', 'snow_1h',\n",
       "       'day_of_week_sin', 'day_of_week_cos', 'year_2018', 'year_2019',\n",
       "       'year_2020', 'year_2021', 'year_2022', 'month_1', 'month_2', 'month_3',\n",
       "       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
       "       'month_10', 'month_11', 'month_12', 'day_1', 'day_2', 'day_3', 'day_4',\n",
       "       'day_5', 'day_6', 'day_7', 'day_8', 'day_9', 'day_10', 'day_11',\n",
       "       'day_12', 'day_13', 'day_14', 'day_15', 'day_16', 'day_17', 'day_18',\n",
       "       'day_19', 'day_20', 'day_21', 'day_22', 'day_23', 'day_24', 'day_25',\n",
       "       'day_26', 'day_27', 'day_28', 'day_29', 'day_30', 'day_31',\n",
       "       'Bumper Cars', 'Bungee Jump', 'Circus Train', 'Crazy Dance',\n",
       "       'Dizzy Dropper', 'Drop Tower', 'Flying Coaster', 'Free Fall',\n",
       "       'Giant Wheel', 'Giga Coaster', 'Go-Karts', 'Haunted House',\n",
       "       'Himalaya Ride', 'Inverted Coaster', 'Kiddie Coaster', 'Merry Go Round',\n",
       "       'Oz Theatre', 'Rapids Ride', 'Roller Coaster', 'Spinning Coaster',\n",
       "       'Spiral Slide', 'Superman Ride', 'Swing Ride', 'Vertical Drop',\n",
       "       'Water Ride', 'Zipline', 'FACILITY_NAME', 'attendance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waiting_times.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiting_times.to_csv(\"data/processed_data_waiting_times.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "waiting_times = pd.read_csv(\"data/processed_data_waiting_times.csv\")\n",
    "\n",
    "\n",
    "waiting_times.drop(columns=[\"DEB_TIME\", \"DEB_TIME_HOUR\", \"FACILITY_NAME\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:22.98672\n",
      "[100]\tvalidation_0-rmse:12.01347\n",
      "[200]\tvalidation_0-rmse:11.48751\n",
      "[300]\tvalidation_0-rmse:11.48942\n",
      "[400]\tvalidation_0-rmse:11.51301\n",
      "[500]\tvalidation_0-rmse:11.57714\n",
      "[600]\tvalidation_0-rmse:11.60122\n",
      "[700]\tvalidation_0-rmse:11.62659\n",
      "[800]\tvalidation_0-rmse:11.65505\n",
      "[900]\tvalidation_0-rmse:11.72716\n",
      "[999]\tvalidation_0-rmse:11.77313\n",
      "Final Validation RMSE: 11.773126602172852\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the results\n",
    "train_rmse_list = []\n",
    "val_rmse_list = []\n",
    "\n",
    "waiting_times[\"date\"] = pd.to_datetime(waiting_times[\"date\"])\n",
    "cutoff_date = waiting_times[\"date\"].max() - pd.DateOffset(months=3)\n",
    "train_set = waiting_times[waiting_times[\"date\"] < cutoff_date]\n",
    "val_set = waiting_times[waiting_times[\"date\"] >= cutoff_date]\n",
    "X_train = train_set.drop(columns=[\"WAIT_TIME_MAX\", \"date\"])\n",
    "y_train = train_set[\"WAIT_TIME_MAX\"]\n",
    "X_val = val_set.drop(columns=[\"WAIT_TIME_MAX\", \"date\"])\n",
    "y_val = val_set[\"WAIT_TIME_MAX\"]\n",
    "\n",
    "# Train the final model on the entire training data\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    subsample=0.9,\n",
    "    gamma=0.1,\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "\n",
    "# Predict on the final validation set\n",
    "y_pred_final = model.predict(X_val)\n",
    "\n",
    "np.where(y_pred_final < 0, 0, y_pred_final)\n",
    "\n",
    "# Evaluate the final model\n",
    "final_rmse = root_mean_squared_error(y_val, y_pred_final)\n",
    "print(f\"Final Validation RMSE: {final_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = X_val.drop(\n",
    "    columns=[\n",
    "        \"temp\",\n",
    "        \"humidity\",\n",
    "        \"wind_speed\",\n",
    "        \"clouds_all\",\n",
    "        \"rain_1h\",\n",
    "        \"snow_1h\",\n",
    "        \"day_of_week_sin\",\n",
    "        \"day_of_week_cos\",\n",
    "        \"attendance\",\n",
    "    ]\n",
    ")\n",
    "pred_df[\"WAIT_TIME_MAX\"] = y_pred_final\n",
    "pred_df.loc[pred_df[\"WAIT_TIME_MAX\"] < 0, \"WAIT_TIME_MAX\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'WAIT_TIME_MAX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/work/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'WAIT_TIME_MAX'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[0;32m----> 3\u001b[0m final_rmse \u001b[38;5;241m=\u001b[39m root_mean_squared_error(y_val, \u001b[43mpred_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWAIT_TIME_MAX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m final_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_val, pred_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAIT_TIME_MAX\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Validation RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_rmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/work/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/work/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'WAIT_TIME_MAX'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "final_rmse = root_mean_squared_error(y_val, pred_df[\"WAIT_TIME_MAX\"])\n",
    "final_mae = mean_absolute_error(y_val, pred_df[\"WAIT_TIME_MAX\"])\n",
    "print(f\"Final Validation RMSE: {final_rmse}\")\n",
    "print(f\"Final Validation MAE: {final_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3387/774666253.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_df['ride'] = pred_df[['Bumper Cars', 'Bungee Jump', 'Circus Train', 'Crazy Dance',\n",
      "/tmp/ipykernel_3387/774666253.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_df[\"date\"] = pd.to_datetime(pred_df[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "waiting_times = pd.read_csv(\"data/processed_data_waiting_times.csv\")\n",
    "pred_df = waiting_times[\n",
    "    [\n",
    "        \"DEB_TIME\",\n",
    "        \"date\",\n",
    "        \"Bumper Cars\",\n",
    "        \"Bungee Jump\",\n",
    "        \"Circus Train\",\n",
    "        \"Crazy Dance\",\n",
    "        \"Dizzy Dropper\",\n",
    "        \"Drop Tower\",\n",
    "        \"Flying Coaster\",\n",
    "        \"Free Fall\",\n",
    "        \"Giant Wheel\",\n",
    "        \"Giga Coaster\",\n",
    "        \"Go-Karts\",\n",
    "        \"Haunted House\",\n",
    "        \"Himalaya Ride\",\n",
    "        \"Inverted Coaster\",\n",
    "        \"Kiddie Coaster\",\n",
    "        \"Merry Go Round\",\n",
    "        \"Oz Theatre\",\n",
    "        \"Rapids Ride\",\n",
    "        \"Roller Coaster\",\n",
    "        \"Spinning Coaster\",\n",
    "        \"Spiral Slide\",\n",
    "        \"Superman Ride\",\n",
    "        \"Swing Ride\",\n",
    "        \"Vertical Drop\",\n",
    "        \"Water Ride\",\n",
    "        \"Zipline\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "pred_df[\"ride\"] = pred_df[\n",
    "    [\n",
    "        \"Bumper Cars\",\n",
    "        \"Bungee Jump\",\n",
    "        \"Circus Train\",\n",
    "        \"Crazy Dance\",\n",
    "        \"Dizzy Dropper\",\n",
    "        \"Drop Tower\",\n",
    "        \"Flying Coaster\",\n",
    "        \"Free Fall\",\n",
    "        \"Giant Wheel\",\n",
    "        \"Giga Coaster\",\n",
    "        \"Go-Karts\",\n",
    "        \"Haunted House\",\n",
    "        \"Himalaya Ride\",\n",
    "        \"Inverted Coaster\",\n",
    "        \"Kiddie Coaster\",\n",
    "        \"Merry Go Round\",\n",
    "        \"Oz Theatre\",\n",
    "        \"Rapids Ride\",\n",
    "        \"Roller Coaster\",\n",
    "        \"Spinning Coaster\",\n",
    "        \"Spiral Slide\",\n",
    "        \"Superman Ride\",\n",
    "        \"Swing Ride\",\n",
    "        \"Vertical Drop\",\n",
    "        \"Water Ride\",\n",
    "        \"Zipline\",\n",
    "    ]\n",
    "].idxmax(axis=1)\n",
    "\n",
    "\n",
    "pred_df[\"date\"] = pd.to_datetime(pred_df[\"date\"])\n",
    "pred_df = pred_df[pred_df[\"date\"] >= cutoff_date]\n",
    "pred_df[\"pred\"] = y_pred_final\n",
    "pred_df.loc[pred_df[\"pred\"] < 0, \"pred\"] = 0\n",
    "\n",
    "pred_df.drop(\n",
    "    columns=[\n",
    "        \"Bumper Cars\",\n",
    "        \"Bungee Jump\",\n",
    "        \"Circus Train\",\n",
    "        \"Crazy Dance\",\n",
    "        \"Dizzy Dropper\",\n",
    "        \"Drop Tower\",\n",
    "        \"Flying Coaster\",\n",
    "        \"Free Fall\",\n",
    "        \"Giant Wheel\",\n",
    "        \"Giga Coaster\",\n",
    "        \"Go-Karts\",\n",
    "        \"Haunted House\",\n",
    "        \"Himalaya Ride\",\n",
    "        \"Inverted Coaster\",\n",
    "        \"Kiddie Coaster\",\n",
    "        \"Merry Go Round\",\n",
    "        \"Oz Theatre\",\n",
    "        \"Rapids Ride\",\n",
    "        \"Roller Coaster\",\n",
    "        \"Spinning Coaster\",\n",
    "        \"Spiral Slide\",\n",
    "        \"Superman Ride\",\n",
    "        \"Swing Ride\",\n",
    "        \"Vertical Drop\",\n",
    "        \"Water Ride\",\n",
    "        \"Zipline\",\n",
    "        \"date\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "pred_df.to_csv(\"data/waiting_time_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation RMSE: 11.723296165466309\n",
      "Final Validation MAE: 5.6422038078308105\n"
     ]
    }
   ],
   "source": [
    "final_rmse = root_mean_squared_error(y_val, pred_df[\"pred\"])\n",
    "final_mae = mean_absolute_error(y_val, pred_df[\"pred\"])\n",
    "print(f\"Final Validation RMSE: {final_rmse}\")\n",
    "print(f\"Final Validation MAE: {final_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
